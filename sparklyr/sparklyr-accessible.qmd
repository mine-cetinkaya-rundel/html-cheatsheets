---
title: "Data Science in Spark with Sparklyr :: Cheat Sheet"
format: 
  html:
    toc: true
    highlight-style: a11y-dark
editor: visual
---

## Intro

sparklyr is an R interface for **Apache Spark**, it provides a complete **dplyr** backend and the option to query directly using **Spark SQL** statement. With **sparklyr**, you can orchestrate distributed machine learning using either **Spark's MLlib** or **H2O** Sparkling Water. Starting with **version 1.044**, **RStudio Desktop**, **Server and Pro include integrated support for the sparklyr package**. You can create and manage connections to Spark clusters and local Spark instances from inside the IDE.

### RStudio Integrates with sparklyr

TODO Screenshots

Expand to read about the sparklyr features in the RStudio IDE.

#### Sparklyr features in the RStudio IDE

-   Open connection log
-   Disconnect
-   Open the Spark UI
-   Spark & Hive Tables
-   Preview 1K rows

## Cluster Deployment

In a managed cluster, the driver node (RStudio, Spark, Hive) connects to the cluster manager (Yarn, Mesos) which connects to the worker nodes (Spark).

In a stand alone cluster the driver node (RStudio, Spark) connects directly to the worker nodes (Spark).

## Data Science Toolchain with Spark + sparklyr

1.  Import
    -   Export an R DataFrame

    -   Read a file

    -   Read existing Hive table
2.  Tidy/Wrangle
    -   dplyr verb

    -   Direct Spark SQL (DBI)

    -   SDF function (Scala API)
3.  Understand
    -   Transform - Transformer function

    -   Visualize - Collect data into R for plotting

    -   Model - Spark MLlib and H2O Extension
4.  Communicate
    -   Collect data into R

    -   Share plots, documents, and apps

## Getting Started

### Local Mode (no cluster required)

1.  Install a local version of Spark:

    ```{r}
    #| eval: false
    spark_install("2.0.1")
    ```

2.  Open a connection:

    ```{r}
    #| eval: false
    sc <- spark_connect(master = "local")
    ```

### On a Mesos Managed Cluster

1.  Install RStudio Server or Pro on one of the existing nodes

2.  Locate path to the cluster's Spark directory

3.  Open a connection

    ```{r}
    #| eval: false

    spark_connect(master = "[mesos URL]",
                  version = "1.6.2", 
                  spark_home = [Cluster’s Spark path])
    ```

### Using Livy (Experimental)

1.  The Livy REST application should be running on the cluster

2.  Connect to the cluster

    ```{r}
    #| eval: false

    sc <- spark_connect(method = "livy", 
                        master = "http://host:port")
    ```

### On a Yarn Managed Cluster

1.  Install RStudio Server or RStudio Pro on one of the existing nodes, preferably an edge node

2.  Locate path to the cluster's Spark Home Directory, it normally is `/usr/lib/spark`

3.  Open a connection

    ```{r}
    #| eval: false

    spark_connect(master="yarn-client", 
                  version = "1.6.2", 
                  spark_home = [Cluster’s Spark path])
    ```

### On a Spark Standaline Cluster

1.  Install RStudio Server or RStudio Pro on one of the existing nodes or a server in the same LAN

2.  Install a local version of Spark:

    ```{r}
    #| eval: false
    spark_install(version = "2.0.1")
    ```

3.  Open a connection

    ```{r}
    #| eval: false
    spark_connect(master="spark:// host:port",
                  version = "2.0.1", 
                  spark_home = spark_home_dir())
    ```

## Tuning Spark

### Example Configuration

```{r}
#| eval: false

config <- spark_config() 
config$spark.executor.cores <- 2
config$spark.executor.memory <- "4G" 
sc <- spark_connect (master="yarn-client", config = config, version = "2.0.1")
```

### Important Tuning Parameters (with defaults)

-   `spark.yarn.am.cores`
-   `spark.yarn.am.memory`: 512m
-   `spark.network.timeout`: 120s
-   `spark.executor.memory`: 1g
-   `spark.executor.cores`: 1
-   `spark.executor.instances`
-   `spark.executor.extraJavaOptions`
-   `spark.executor.heartbeatInterval`: 10s
-   `sparklyr.shell.executor-memory`
-   `sparklyr.shell.driver-memory`

## Using sparklyr

```{r}
#| eval: false

library(sparklyr)
library(dplyr)
library(ggplot2)
library(tidyr)
set.seed(100)

#Install Spark locally
spark_install("2.0.1")

# Connect to local version
sc <- spark_connect(master = "local")

# Copy data to Spark memory
import_iris <- copy_to(sc, 
                       iris, 
                       "spark_iris", 
                       overwrite = TRUE)

# Partition data
partition_iris <- sdf_partition(import_iris,
                                training = 0.5, 
                                testing = 0.5)

#Create a hive metadata for each partition

sdf_register(partition_iris,
             c("spark_iris_training", "spark_iris_test"))
      
spark_connect(master = "[mesos URL]", 
              version = "1.6.2", spark_home = [Cluster’s Spark path])

tidy_iris <- tbl(sc, "spark_iris_training") %>% 
  select(Species, Petal_Length, Petal_Width)

# Spark ML Decision Tree Model
model_iris <- tidy_iris %>%
  ml_decision_tree(response = "Species",
                   features = c("Petal_Length", "Petal_Width"))

# Create reference to Spark table
test_iris <- tbl(sc, "spark_iris_test")

# Bring data back into R memory for plotting
pred_iris <- sdf_predict(model_iris, test_iris) %>% 
  collect

pred_iris %>% inner_join(data.frame(prediction = 0:2, lab = model_iris$model.parameters$labels)) %>%
  ggplot(aes(Petal_Length, Petal_Width, col = lab)) + geom_point()

# Disconnect
spark_disconnect(sc)
```

<!-- page 2 -->

## Reactivity

### Copy a Data Frame Into Spark

-   `sdf_copy_to(sc, x, name, memory, repartition, overwrite)`

    ```{r}
    #| eval: false

    sdf_copy_to(sc, iris, "spark_iris")
    ```

### Import Into Spark From a File

Arguments that apply to all functions: `sc`, `name`, `path`, `options = list()`, `repartition = 0`, `memory = TRUE`, `overwrite = TRUE`

-   `spark_read_csv(header = TRUE, columns = NULL, infer_schema = TRUE, delimiter = ",", quote = "\"", escape = "\\", charset = "UTF-8", null_value = NULL)`

-   `spark_read_json()`

-   `spark_read_parquet()`

### Spark SQL Commands

-   `DBI::dbWriteTable(conn, value)`

    ```{r}
    #| eval: false

    DBI::dbWriteTable(sc, "spark_iris", iris)
    ```

### From a Table in Hive

-   `tbl_cache(sc, name, force = TRUE)`: Loads the table into memory

    ```{r}
    #| eval: false
    my_var <- tbl_cache(sc, name= "hive_iris")
    ```

-   `dplyr::tbl(scr, ...)`: Creates a reference to the table without loading it into memory

    ```{r}
    #| eval: false
    my_var <- dplyr::tbl(sc, name= "hive_iris")
    ```

## Wrangle

### Spark SQL via dplyer Verbs

-   Translates into Spark SQL statements:

    ```{r}
    #| eval: false
      
    my_table <- my_var %>% 
      filter(Species=="setosa") %>% 
      sample_n(10)
    ```

### Direct Spark SQL Commands

-   `DBI::dbGetQuery(conn, statement)`

    ```{r}
    #| eval: false
    my_table <- DBI::dbGetQuery(sc, "SELECT * FROM iris LIMIT 10")
    ```

### Scala API via SDF Functions

-   `sdf_mutate(.data)`: Works like dplyr mutate function

-   `sdf_partition(x, ..., weights = NULL, seed = sample (.Machine$integer.max, 1))`

    ```{r}
    #| eval: false
    sdf_partition(x, training = 0.5, test = 0.5) sdf_register(x, name = NULL)
    ```

-   `sdf_register(x, name = NULL)`: Gives a Spark DataFrame a table name

-   `sdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)`

-   `sdf_sort(x, columns)`: Sorts by \>=1 columns in ascending order

-   `sdf_with_unique_id(x, id = "id")`

-   `sdf_predict(object, newdata)`: Spark DataFrame with predicted values

### ML Transformers

Example:

```{r}
#| eval: false
ft_binarizer(my_table,
             input.col="Petal_Length", 
             output.col="petal_large", 
             threshold=1.2)
```

Arguments that apply to all functions: `x`, `input.col = NULL`, `output.col = NULL`

-   `ft_binarizer(threshold = 0.5)`: Assigned values based on threshold

-   `ft_bucketizer(splits)`: Numeric column to discretized column

-   `ft_discrete_cosine_transform(inverse = FALSE)`: Time domain to frequency domain

-   `ft_elementwise_product(scaling.col)`: Element-wise product between 2 cols

-   `ft_index_to_string()`: Index labels back to label as strings

-   `ft_one_hot_encoder()`: Continuous to binary vectors

-   `ft_quantile_discretizer(n.buckets=5L)`: Continuous to binned categorical values

-   `ft_sql_transformer(sql)`

-   `ft_string_indexer(params = NULL)`: Column of labels into a column of label indices

-   `ft_vector_assembler()`: Combine vectors into single row-vector

## Visulize & Communicate

### Download Data to R Memory

Example:

```{r}
#| eval: false

r_table <- collect(my_table) 
plot(Petal_Width~Petal_Length, 
     data=r_table)
```

-   `dplyr::collect(x)`: Download a Spark DataFrame to an R DataFrame

-   `sdf_read_column(x, column)`: Returns contents of a single column to R

### Save From Spark to File System

Arguments that apply to all functions: `x`, `path`

-   `spark_read_csv( header = TRUE, delimiter = ",", quote = "\"", escape = "\\", charset = "UTF-8", null_value = NULL)`

-   `spark_read_json(mode = NULL)`

-   `spark_read_parquet(mode = NULL)`

## Reading & Writing from Apache Spark

Write to Spark, from R with `sdf_copy_to()`, `dplyr::copy_to()`, or `DBI::sbWriteTable()`.

Read from Spark, to R with `sdf_collect()`, `dplyr::collect()`, `sdf_read_column`.

------------------------------------------------------------------------

Write to Spark, from Hive with `tbl_cache()` or `dplyr::tbl()`.

------------------------------------------------------------------------

Write to Spark from the file system with `spark_read_<fmt>()`.

Read from Spark to the file system with `spark_write_<fmt>()`.

## Extensions

Create an R package that calls the full Spark API & provide interfaces to Park packages.

### Core Types

-   `spark_connection()`: Connection between R and the Spark shell process

-   `spark_jobj()`: Instance og a remote Spark object

-   `spark_dataframe()`: Instance of a remote Spark DataFrame object

### Call Spark From R

-   `invoke()`: Call a method on a Java object

-   `invoke_new()`: Create a new object by invoking a constructor

-   `invoke_static()`: Call a static method on an object

### Machine Learning Extensions

-   `ml_create_dummy_variables()`

-   `ml_prepare_dataframe()`

-   `ml_prepare_response_features_intercept()`

-   `ml_options()`

-   `ml_model()`

## Model (MLlib)

Example:

```{r}
#| eval: false
ml_decision_tree(my_table, 
                 response = "Species", features = c("Petal_Length" , "Petal_Width"))
```

-   `ml_als_factorization(x, user.column = "user", rating.column = "rating", item.column = "item", rank = 10L, regularization.parameter = 0.1, iter.max = 10L, ml.options = ml_options())`

-   `ml_decision_tree(x, response, features, max.bins = 32L, max.depth = 5L, type = c("auto", "regression", "classification"), ml.options = ml_options())`: Same options for: ml_gradient_boosted_trees

-   `ml_generalized_linear_regression(x, response, features, intercept = TRUE, family = gaussian(link = "identity"), iter.max = 100L, ml.options = ml_options())`

-   `ml_kmeans(x, centers, iter.max = 100, features = dplyr::tbl_vars(x), compute.cost = TRUE, tolerance = 1e-04, ml.options = ml_options())`

-   `ml_lda(x, features = dplyr::tbl_vars(x), k = length(features), alpha = (50/k) + 1, beta = 0.1 + 1, ml.options = ml_options())`

-   `ml_linear_regression(x, response, features, intercept = TRUE, alpha = 0, lambda = 0, iter.max = 100L, ml.options = ml_options())`: Same options for: ml_logistic_regression

-   `ml_multilayer_perceptron(x, response, features, layers, iter.max = 100, seed = sample(.Machine$integer.max, 1), ml.options = ml_options())`

-   `ml_naive_bayes(x, response, features, lambda = 0, ml.options = ml_options())`

-   `ml_one_vs_rest(x, classifier, response, features, ml.options = ml_options())`

-   `ml_pca(x, features = dplyr::tbl_vars(x), ml.options = ml_options())`

-   `ml_random_forest(x, response, features, max.bins = 32L, max.depth = 5L, num.trees = 20L, type = c("auto", "regression", "classification"), ml.options = ml_options())`

-   `ml_survival_regression(x, response, features, intercept = TRUE,censor = "censor", iter.max = 100L, ml.options = ml_options())`

-   `ml_binary_classification_eval(predicted_tbl_spark, label, score, metric = "areaUnderROC")`

-   `ml_classification_eval(predicted_tbl_spark, label, predicted_lbl, metric = "f1")`

-   `ml_tree_feature_importance(sc, model)`
